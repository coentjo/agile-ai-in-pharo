---
title: "Understanding the Perceptron"
---

## The Biological Inspiration: From Brain Neurons to Artificial Intelligence

The Perceptron represents one of the most fundamental concepts in artificial intelligence, drawing its inspiration directly from the human brain's neural structure. This groundbreaking idea was first introduced in 1943 by Warren S. McCulloch and Walter Pitts in their seminal paper 'A Logical Calculus of the Ideas Immanent in Nervous Activity', where they proposed a mathematical model of biological neurons.

![A typical biological neuron structure](attach/TypicalNeuron.png)

## From Biology to Machine: Implementing a Perceptron

A Perceptron's architecture elegantly mirrors its biological counterpart through three key components: `inputs`, `weights`, and a `bias`. Each input connection has an associated weight that determines its relative importance, while the bias helps adjust the Perceptron's overall sensitivity to activation.

![Perceptron's architectural diagram](attach/Perceptron01.png)

Let's explore a practical example with three inputs. We'll call our input values `x1`, `x2`, and `x3`, with their corresponding weights `w1`, `w2`, and `w3`. The Perceptron processes these inputs in two steps:

1. First, it calculates a `weighted sum` and adds the bias:
   `z := w1*x1 + w2*x2 + w3*x3 + bias`

2. Then, it applies what we call an **activation function** to produce the final output: let's use a very simpel one, called a Step function: 

$\begin{cases}
\text{Output is 1 if } z > 0 \\
\text{Output is 0 if } z \leq 0
\end{cases}$

which determines the final output.


Keep in mind that the number of inputs for a Perceptron can vary. 

