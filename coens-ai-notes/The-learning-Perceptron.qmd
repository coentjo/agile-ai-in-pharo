---
title: "Teaching a Perceptron: The Learning Process"
---

## Introduction to Perceptron Learning

One of the most fascinating aspects of Perceptrons is their ability to learn from examples. Instead of manually setting weights and bias, we can train a Perceptron to discover the optimal parameters through a process called supervised learning.

## The Learning Algorithm

The learning process follows these key steps:

1. Start with random weights and bias
2. Present a training example
3. Compare the Perceptron's output with the desired output
4. Adjust the weights and bias based on the error
5. Repeat with more examples until performance is satisfactory

### Mathematical Foundation

The weight update rule is elegantly simple:
```
new_weight = current_weight + learning_rate * error * input
```

Where:
- `learning_rate` is a small number (like 0.1) that controls how big each adjustment is
- `error` is the difference between desired and actual output (1 or -1)
- `input` is the input value for that weight

## Implementing Learning

Here's how we create a learning Perceptron:

```pharo
learningPerceptron := Neuron new
    step;
    learningRate: 0.1;
    initialize.  "Sets random initial weights"
```

### Training Process

To train the Perceptron, we present examples with their desired outputs:

```pharo
"Training for AND gate behavior"
trainingData := #(
    ((0 0) 0)
    ((0 1) 0)
    ((1 0) 0)
    ((1 1) 1)
).

trainingData do: [:example |
    inputs := example first.
    desiredOutput := example second.
    learningPerceptron train: inputs desiredOutput: desiredOutput
].
```

## Visualizing the Learning Process

As the Perceptron learns, its decision boundary gradually moves to the correct position. You can monitor this progress by:

1. Tracking the error rate over time
2. Visualizing the decision boundary's movement
3. Testing the Perceptron with new examples

## Practical Considerations

For successful learning:
- Ensure your training data is representative
- Consider using multiple training epochs (complete passes through the data)
- Monitor for convergence (when the weights stabilize)
- Be aware that not all problems are linearly separable

In the next chapter, we'll explore the limitations of what a single Perceptron can learn, which will lead us naturally to the need for more complex neural networks.
