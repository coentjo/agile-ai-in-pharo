---
title: "The Mathematics Behind Neural Networks"
---

## Understanding the Magic

While neural networks might seem magical, they're built on solid mathematical foundations. 
Let's demystify (a bit of) how they actually work under the hood.

## The Building Blocks

### 1. Neurons and Weights

To be formally correct we should say `artificial neuron` to distinguish them from 
`biological neurons` like we have in our brain. A neuron normally has inputs: $1$, or $2$, or $\cdots$



Each neuron performs two key operations:
1. Weighted sum of inputs.
2. Activation function: $a = f(z)$


### 2. Activation Functions


Common activation functions include:

1. Sigmoid: $f(x) = \frac{1}{1 + e^{-x}}$
   - Outputs between 0 and 1
   - Useful for probability predictions

2. ReLU (Rectified Linear Unit): $f(x) = \max(0, x)$
   - Simple and efficient
   - Helps prevent vanishing gradients

3. Tanh: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
   - Outputs between -1 and 1
   - Often better than sigmoid for hidden layers

## The Learning Process

### 1. Forward Propagation

Information flows through the network.


### 2. Loss Calculation

Measure the network's Error and Backpropagation

- What is the output?
- What would be my desired output? 

The smaller the difference between the output I got and the output I desired, the better the output of my model is. 
This difference is calculated with a so-called Loss function. 
Backpropagation is an algorithm that helps make that difference small. 
When backpropagation is performed we call that Training the AI model.


